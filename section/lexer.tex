\section{Lexer}
The lexical analyzer has been written using flex.
It has the following responsabilities:
\begin{itemize}
\item Verify that the input stream is lexically correct;
\item Produce the token stream for the parser;
\item Set the \verb|bison/flex| variable \verb|yylloc| -- this
variable contains the information about the first column, last column,
first row and last row of the token read. More details on its usage in
bison can be read in section \ref{sec:parser}.
\item Raise an error in case of a fract constant with denominator equal
to $0$ -- it is simply not considered as a valid lexeme.
\end{itemize}

\subsection{Lexemes}
We chose to define an identifier for each lexem, this enable us to change the
syntax of the F language without affecting its semantics and the other
components. Basically, the goal of this choice is to have something similar to
a C macro or a placeholder for the syntax symbols of the F language.
For example we have defined \verb|SUM| which is a placeholder for \verb|+|, so
if tomorrow we decide to use \verb|plus| to perform the arithmetic sum we can do
it transparently to the rest of the FAC code. This provides a great flexibility
for lexer symbols.

\subsection{Comments}
We have also implemented the regex which checks the comments. So it is possible
to write comments in the source code of F programs. They will be simply 
discarded during the phase of lexical analysis.

\subsection{Options}
We have used some flex options which enable optimizations
and to track the source code lines during the lexical analysis. Indeed, the
generated lexer does not contain a compressed table but a larger one which can
be accessed faster than the compressed one. Also, we used the align option to
notify flex to align memory access location of the generated table when
possible. Unfortunately, the line tracking option kills the performances.
In this case we have opted for a cleaner solution than an efficient one.

\subsection{Modularity}
The lexer can be generated on the fly. Indeed, each regex and each rule are
contained in a file which reflects respectively the regex and the rule purpose.
The regexs and the rules are automatically assembled by a Python script.
The script reads two configuration files written in json. These files specify
the order in which the regexs and the rules have to be written in the
\path{lexer.fl} file. Thus we can simply modify the configuration files to 
change the precedence of the lexer rules\footnote{Precedence rules are 
applied only in case of a tie.}.
